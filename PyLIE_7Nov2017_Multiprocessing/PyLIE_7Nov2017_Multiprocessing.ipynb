{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PyLIE: Multiprocessing\n",
    "### 7 November 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Setup\n",
    "* As usual, I suggest creating a `conda env` or `virtualenv` for this \"project\"<br>\n",
    "* `pip install numpy pandas pandas_datareader jupyter dask snakeviz`<br>\n",
    "* This will get you most of what you need for this demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All examples are taken from Enthought's tutorial at SciPY 2017. The tutorial and materials can be found [here](https://github.com/pydata/parallel-tutorial#installation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First we need to generate some data to play around with. To do that, clone the repo, start up this notebook (on your local machine, not through github), and run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "!python ./prep.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Data is just interpolated stock data in JSON and CSV formats. Now that the data is populated, we can work on some examples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Map\n",
    "<img src=\"map.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `map` is used in just about every multiprocessing framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `map` is a concise version of a for-loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def f(number: int) -> int:\n",
    "    return number - (number - 1)\n",
    "\n",
    "# Simple for-loop\n",
    "tmpLstA = []\n",
    "for i in range(5):\n",
    "    tmpLstA.append(f(i))\n",
    "\n",
    "# List comprehension\n",
    "tmpListB = [f(i) for i in range(5)]\n",
    "\n",
    "# map applies a function to each item of iterable\n",
    "tmpListC = map(f, range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## `map` with `concurrent.futures`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### First, non-parallel sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "### Sequential code\n",
    "\n",
    "import time\n",
    "\n",
    "results = []\n",
    "\n",
    "for i in range(8):\n",
    "    time.sleep(1)\n",
    "    results.append(i + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now, parallel sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# multiprocessing code\n",
    "\n",
    "import sys\n",
    "\n",
    "WINDOWS = sys.platform.startswith('win')\n",
    "\n",
    "if WINDOWS:\n",
    "    import pip\n",
    "    pip.main(['install', 'loky'])\n",
    "    from loky import ProcessPoolExecutor as Pool\n",
    "else:\n",
    "    from concurrent.futures import ProcessPoolExecutor as Pool\n",
    "    \n",
    "e = Pool()\n",
    "\n",
    "def slowinc(x):\n",
    "    time.sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "results = list(e.map(slowinc, range(8)))\n",
    "e.shutdown(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## More appropriate example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "We are going to convert the data in 'HDF' format, since HDF format is more performant that JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext snakeviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We need to walk our files to figure ID which files we will be converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "filenames = sorted(glob(os.path.join('.', 'data', 'json', '*.json')))  # ../data/json/*.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Regular sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%snakeviz\n",
    "\n",
    "import json\n",
    "\n",
    "### Sequential code\n",
    "\n",
    "for fn in filenames:\n",
    "    with open(fn) as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    out_filename = fn[:-5] + '.h5'\n",
    "    df.to_hdf(out_filename, '/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def converter(fn):\n",
    "    with open(fn) as f:\n",
    "        df = pd.DataFrame([json.loads(line) for line in f])\n",
    "    \n",
    "    out_filename = fn[:-5] + '.h5'\n",
    "    df.to_hdf(f'{fn[:-5]}.h5', '/data')\n",
    "\n",
    "e = Pool()\n",
    "\n",
    "list(e.map(converter, filenames))\n",
    "e.shutdown(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Non-parallel with optimized library\n",
    "Sometimes it is the libraries you use that can be a problem. The famous example is `json` vs `ujson`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import ujson\n",
    "\n",
    "for fn in filenames:\n",
    "    with open(fn) as f:\n",
    "        data = [ujson.loads(line) for line in f]\n",
    "        \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    out_filename = fn[:-5] + '.h5'\n",
    "    df.to_hdf(out_filename, '/data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now, parallel with optimized libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def converter(fn):\n",
    "    with open(fn) as f:\n",
    "        df = pd.DataFrame([ujson.loads(line) for line in f])\n",
    "    \n",
    "    out_filename = fn[:-5] + '.h5'\n",
    "    df.to_hdf(f'{fn[:-5]}.h5', '/data')\n",
    "\n",
    "e = Pool()\n",
    "\n",
    "list(e.map(converter, filenames))\n",
    "e.shutdown(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Submit\n",
    "<img src=\"submit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* `submit` allows the developer fine-grain control of parallel execution.\n",
    "* Most of the time, not necessary\n",
    "* But, when needed, allows for conditionals to control the flow of processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparable to a if-else for-loop\n",
    "```python\n",
    "outputs = {}\n",
    "for a in L1:\n",
    "    for b in L2:\n",
    "        if a > b:\n",
    "            outputs[a, b] = f(a, b)\n",
    "        else:\n",
    "            outputs[a, b] = g(a, b)\n",
    "```\n",
    "### vs submit\n",
    "```python\n",
    "\n",
    "e = concurrent.futures.ThreadPoolExecutor()\n",
    "futures = {}\n",
    "for a in L1:\n",
    "    for b in L2:\n",
    "        if a > b:\n",
    "            futures[a, b] = e.submit(f, a, b)  # submit to background thread\n",
    "        else:\n",
    "            futures[a, b] = e.submit(g, a, b)  # submit to background thread\n",
    "\n",
    "outputs = {k: v.result() for k, v in futures}  # block until all finished\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Back to the data\n",
    "Consider the HDF5 data we converted. These are just some stock data. Now, what if we wanted to do a pair-wise correlation of all $n\\times(n-1)$ possibilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sequential correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# File loading is already parallel thanks to pandas C library\n",
    "\n",
    "filenames = sorted(glob(os.path.join('.', 'data', 'json', '*.h5')))  # ../data/json/*.json\n",
    "\n",
    "series = {}\n",
    "for fn in filenames:\n",
    "    series[fn] = pd.read_hdf(fn)['close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Nested for-loop\n",
    "\n",
    "results = {}\n",
    "\n",
    "for a in filenames:\n",
    "    for b in filenames:\n",
    "        if a != b:\n",
    "            results[a, b] = series[a].corr(series[b])\n",
    "            \n",
    "\n",
    "# data reduction is a simple one-off calculation that doesn't require parallelization\n",
    "((a, b), corr) = max(results.items(), key=lambda kv: kv[1])\n",
    "print(\"%s matches with %s with correlation %f\" % (a, b, corr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Threaded Correlation\n",
    "* To attack this problem, we want to tackle the part that would have the biggest trade-off. This would be the nested for-loop.\n",
    "* Since the for-loop has a conditional, we should use `submit` to handle this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from concurrent.futures import ThreadPoolExecutor as ThreadPool\n",
    "eT = ThreadPool(4)\n",
    "\n",
    "def corr(a, b):\n",
    "    return a.corr(b)\n",
    "\n",
    "futuresT = {}\n",
    "\n",
    "for a in filenames:\n",
    "    for b in filenames:\n",
    "        if a != b:\n",
    "            futuresT[a, b] = eT.submit(corr, series[a], series[b])\n",
    "\n",
    "results = {k: v.result() for k, v in futuresT.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why did we use threads instead of processes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multiprocessed Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "eP = Pool(4)\n",
    "futuresP = {}\n",
    "for a in filenames:\n",
    "    for b in filenames:\n",
    "        if a != b:\n",
    "            futuresP[a, b] = eP.submit(corr, series[a], series[b])\n",
    "\n",
    "results = {k: v.result() for k, v in futuresP.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* `submit` always pushes tasks to the framework *one-by-one*.\n",
    "* Upon submission, a `future` object is returned\n",
    "* The `future` object waits for the result of its task\n",
    "* Then the `.result` provides the outcome. Since these tasks are I/O bound, multiprocessing isn't helpful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### When to thread vs multiprocess?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If one has many isolated objects that need processing without the need for information from the others, `mutliprocessing` is the way to go. These get sent to the CPU/GPU cores for processing. <br>\n",
    "* *Think of this as sending a toy in the mail with the instructions in the box--you never expect to get the toy back*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If one has many objects that need processing, but you need all of them done before you can move on, `threading` is the way to go. These get sent to CPU threads for processing.<br>\n",
    "* *Think of this as going grocery shopping with someone else. While they are getting bread, you can get milk--but you can't check out until both items are in the cart*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Dask is a flexible parallel computing library for analytic computing. As such, it contains \"big data\" collections that make use of primitives like `map`, `filter`, `groupby`, and `join`. Dask cheat sheet can be found [here](\"https://github.com/betteridiot/PYLIE/blob/master/PyLIE_7Nov2017_Multiprocessing/daskcheatsheet.pdf")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "With `dask`, a user submits a series of instructions to the framework that need to be processed. These instructions are turned into a graph that can be visualized to see how the different parts come together. More importantly, these instructions are not processed until `.computer()` is called."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "`dask.bag` is used when dealing with large quantities of semi-structured data like JSON blobs or log files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dask parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import dask\n",
    "dask.set_options(get=dask.threaded.get) # make dask use threads instead of processes\n",
    "import dask.bag as db\n",
    "\n",
    "filenames = sorted(glob(os.path.join('.', 'data', 'json', '*.h5')))\n",
    "\n",
    "# tell dask what iterable we will be working on\n",
    "b = db.from_sequence(filenames)\n",
    "\n",
    "# tell dask that it will then read in those files using pandas\n",
    "series = b.map(lambda fn: pd.read_hdf(fn)['close'])\n",
    "\n",
    "# then tell the sequence of processing steps to take with the data\n",
    "corr = (series.product(series)                               # take the cartesian product of the input\n",
    "              .filter(lambda ab: not (ab[0] == ab[1]).all()) # select all but the diagonal\n",
    "              .map(lambda ab: ab[0].corr(ab[1]))             # find the correlation of all the remaining\n",
    "              .max())                                        # pool the results and find the max\n",
    "\n",
    "# Now tell Dask to do it all\n",
    "result = corr.compute() "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
