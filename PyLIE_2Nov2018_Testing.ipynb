{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Test-Driven Development (TDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(arg):\n",
    "    \"\"\"Short description\n",
    "    \n",
    "    Args:\n",
    "        arg (int): some number\n",
    "    Returns:\n",
    "        (float): another number\n",
    "    \n",
    "    Examples:\n",
    "        >>> foo(7)\n",
    "        14.0\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose:** to verify that the software package functions according to the expectations defined by the requirements/specifications. \n",
    "\n",
    "The overall objective to ***not*** to find every software bug that exists, but to uncover situations that could negatively impact the usability and/or maintainability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of testing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Unit testing\n",
    "    * tests individual units of code with mock dependencies and/or variables\n",
    "* Functional testing\n",
    "    * tests parts (or whole) portions of a code base (superset of unit tests)\n",
    "* Parametric testing\n",
    "    * tests the entire module with parameterized arguments\n",
    "* Fault tolerant testing\n",
    "    * tests the module against illegal or inappropriate variables/dependencies\n",
    "* Integration testing\n",
    "    * tests how well the module plays with others\n",
    "* Regression testing\n",
    "    * retesting entire subsystems/modules/units after other tests to verify no new bugs have been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections.abc as abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_config(config_file, start_top = True):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        config_file (str|fileObject): ...\n",
    "    Yield:\n",
    "        some configuration\n",
    "    Raises:\n",
    "        FileNotFoundError: if the file doesn't exist\n",
    "        SyntaxError: ...\n",
    "    \n",
    "    Examples:\n",
    "        >>> parse_config('spam.cfg')\n",
    "        7\n",
    "        >>> blah = open('spam.cfg')\n",
    "        >>> parse_config(blah)\n",
    "        7\n",
    "        >>> parse_config('dne.txt')\n",
    "        Traceback (most recent call last):\n",
    "            ...\n",
    "        FileDoesNotExistError('...')'\n",
    "        \n",
    "    \"\"\"\n",
    "    if type(config_file) == str:\n",
    "        if os.path.isfile(config_file):\n",
    "            handle = open(config_file)\n",
    "        else:\n",
    "            raise FileNotFoundError\n",
    "    elif isinstance(config_file, io.IOBase):\n",
    "        handle = config_file\n",
    "        handle.seek(0)\n",
    "    else:\n",
    "        raise SyntaxError(\"Didn't provide a valid file\")\n",
    "    for i, line in enumerate(handle):\n",
    "        pass\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(a, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = open('blah.txt', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_io.BufferedReader"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(a, io.IOBase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unit testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scenario:** You write the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(arg):\n",
    "    \"\"\"Does something\n",
    "    \n",
    "    Args:\n",
    "        foo (int): some number\n",
    "    \n",
    "    Returns:\n",
    "        (float): arg divided by half of itself\n",
    "    \n",
    "    Usage:\n",
    "        >>> foo(7)\n",
    "        2.0\n",
    "    \"\"\"\n",
    "    return arg / (arg / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import doctest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding tests in NoName\n",
      "Trying:\n",
      "    foo(7)\n",
      "Expecting:\n",
      "    2.0\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Test the function's doctests\n",
    "doctest.run_docstring_examples(foo, globals(), verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This should make you feel good. It is well documented. It passes its `doctest`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However**, there is some issues if this is your assumption:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0\n",
    "* str\n",
    "* list\n",
    "* anytype other than a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Code Coverage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is a measure that describes the how much of your code is actually *'touched'* by tests in your test suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## TDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='figures/tdd.jpg'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='figures/tdd_diagram.jpg'/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Process:\n",
    "1. Write a failing test\n",
    "2. Write just enough code to pass the test\n",
    "3. Clean-up and de-dupe\n",
    "4. re-run tests\n",
    "5. repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jumbo\n",
    "\n",
    "def test_jumbo_foo():\n",
    "    a = jumbo.foo('./tests/datasets/blah.config'):\n",
    "        assert a == 7\n",
    "    \n",
    "def test_jumbo_foo_dne():\n",
    "    pytest.raises(FileNotFoundError, jumbo.foo('dne.txt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
